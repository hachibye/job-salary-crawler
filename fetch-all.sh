#!/usr/bin/env bash
set -euo pipefail

# --- Safe locale: try several UTF-8s (macOS æ²’æœ‰ C.UTF-8) ---
for L in C.UTF-8 en_US.UTF-8 zh_TW.UTF-8; do
  if locale -a 2>/dev/null | grep -qi "^${L}$"; then
    export LC_ALL="$L" LANG="$L"
    break
  fi
done


# -------- å…ˆè‡ªæˆ‘æ¶ˆæ¯’ï¼šç§»é™¤è…³æœ¬å…§çš„ BOM/CR/NBSPï¼Œé¿å… $KEYWORD é¡éŒ¯èª¤ --------
if [ -w "$0" ]; then
  perl -i -CS -pe 's/\x{FEFF}//g; s/\r$//; s/\x{00A0}/ /g' "$0" 2>/dev/null || true
fi

# --- å®‰å…¨çš„æœ¬åœ°ç’°å¢ƒï¼ˆé¿å…ç·¨ç¢¼äº‚æµï¼‰ ---
export LC_ALL=C.UTF-8 2>/dev/null || true
export LANG=C.UTF-8 2>/dev/null || true

strip_cr_bom() { perl -CS -pe 's/\x{FEFF}//g; s/\r$//; s/\x{00A0}/ /g' ; }

# --- äº’å‹•è¼¸å…¥ ---
read -r -p "è«‹è¼¸å…¥é—œéµå­—ï¼ˆä¾‹å¦‚ï¼šSRE å·¥ç¨‹å¸«ï¼‰: " KEYWORD_RAW
KEYWORD="$(printf '%s' "$KEYWORD_RAW" | strip_cr_bom | sed 's/[[:space:]]*$//')"

read -r -p "æœ€å¤šæŠ“å¹¾é ï¼Ÿ(ç›´æ¥æŒ‰ Enter è¡¨ç¤ºæŠ“å…¨éƒ¨) " MAX_PAGES_INPUT_RAW
MAX_PAGES_INPUT="$(printf '%s' "${MAX_PAGES_INPUT_RAW:-}" | strip_cr_bom)"

# --- å¯èª¿åƒæ•¸ ---
UA_DEFAULT='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'
UA="${UA:-$UA_DEFAULT}"
RO="${RO:-0}"
ORDER="${ORDER:-15}"
ASC="${ASC:-0}"
MODE="${MODE:-s}"
JOBSOURCE="${JOBSOURCE:-2018indexpoc}"

# --- URL ç·¨ç¢¼ ---
urlencode() {
  if command -v jq >/dev/null 2>&1; then
    jq -sRr @uri
  elif command -v python3 >/dev/null 2>&1; then
    python3 - "$@" <<'PY'
import sys, urllib.parse
print(urllib.parse.quote(sys.stdin.read().rstrip("\n")))
PY
  else
    sed 's/ /%20/g'
  fi
}

KEYWORD_ENC="$(printf '%s' "$KEYWORD" | urlencode)"

# --- åŸºåº•æŸ¥è©¢ ---
base_qs="keyword=${KEYWORD_ENC}&ro=${RO}&order=${ORDER}&asc=${ASC}&mode=${MODE}&jobsource=${JOBSOURCE}"
base_api="https://www.104.com.tw/jobs/search/list?${base_qs}"
base_ref="https://www.104.com.tw/jobs/search/?${base_qs}"

# --- å…ˆå–ç¸½é æ•¸ ---
TOTAL="$(curl -s "${base_api}&page=1" \
  -H 'Accept: application/json, text/plain, */*' \
  -H 'Accept-Language: zh-TW,zh;q=0.9' \
  -H "Referer: ${base_ref}&page=1" \
  -H 'Origin: https://www.104.com.tw' \
  -H 'X-Requested-With: XMLHttpRequest' \
  -H "User-Agent: ${UA}" --compressed | jq -r '.data.totalPage // 0')"

if [[ ! "$TOTAL" =~ ^[0-9]+$ || "$TOTAL" -le 0 ]]; then
  echo "æŸ¥ç„¡è³‡æ–™æˆ–è¢«æ“‹ï¼ˆtotalPage=$TOTALï¼‰ã€‚è«‹æ›é—œéµå­—æˆ–æª¢æŸ¥ headersã€‚" >&2
  exit 1
fi

# è¦†å¯«æœ€å¤šé æ•¸ï¼ˆè‹¥ä½¿ç”¨è€…æœ‰è¼¸å…¥ï¼‰
if [[ -n "$MAX_PAGES_INPUT" ]]; then
  if [[ "$MAX_PAGES_INPUT" =~ ^[0-9]+$ && "$MAX_PAGES_INPUT" -gt 0 ]]; then
    (( MAX_PAGES_INPUT < TOTAL )) && TOTAL="$MAX_PAGES_INPUT"
  else
    echo "æœ€å¤šé æ•¸è¼¸å…¥ç„¡æ•ˆï¼Œå°‡æŠ“å…¨éƒ¨ $TOTAL é ã€‚"
  fi
fi

# --- ç”¢ç”Ÿå…©è¡ŒåŸå‰‡çš„ pipet è¦å‰‡ ---
TS="$(date +%Y%m%d_%H%M)"
SAFEKW="$(printf '%s' "$KEYWORD" | tr ' /' '__' | strip_cr_bom)"
OUT_PIPET="104-${SAFEKW}-${TS}.pipet"
OUT_JSONL="104-${SAFEKW}-${TS}.jsonl"

: > "$OUT_PIPET"
for p in $(seq 1 "$TOTAL"); do
  printf "curl '%s&page=%d' -H 'Accept: application/json, text/plain, */*' -H 'Accept-Language: zh-TW,zh;q=0.9' -H 'Referer: %s&page=%d' -H 'Origin: https://www.104.com.tw' -H 'X-Requested-With: XMLHttpRequest' -H 'User-Agent: %s' --compressed\n" \
    "$base_api" "$p" "$base_ref" "$p" "$UA" >> "$OUT_PIPET"
  printf "@this | jq -c '.data.list[] | {date:.appearDate,date_desc:.appearDateDesc,job_id:.jobNo,title:.jobName,company:.custName,industry:.coIndustryDesc,location:.jobAddrNoDesc,address:.jobAddress,mrt:.mrtDesc,landmark:.landmark,salary:.salaryDesc,salaryLow:.salaryLow,salaryHigh:.salaryHigh,apply:.applyDesc,apply_cnt:(.applyCnt|tonumber?),experience:.periodDesc,education:.optionEdu,majors:(if (.major|type==\"array\" and (.major|length>0)) then .major else empty end),source:.jobsource,job_url:(if .link.job? then \"https:\"+.link.job else \"https://www.104.com.tw/job/\"+(.jobNo|tostring) end),company_url:(if .link.cust? then \"https:\"+.link.cust else empty end)} | with_entries(select(.value!=null and (((.value|type)==\"string\" and .value!=\"\") or ((.value|type)==\"number\") or ((.value|type)==\"array\" and (.value|length>0)) or ((.value|type)==\"object\" and (.value|length>0)))))'\n\n" >> "$OUT_PIPET"
done

# å»é™¤ pipet æª”ä¸­çš„ BOM/CRï¼Œé¿å… "Found block <nil)"ï¼Œä¸¦ç¢ºèªé¦–å­—
perl -i -CS -pe 's/\x{FEFF}//g; s/\r$//' "$OUT_PIPET"
first_char="$(head -c1 "$OUT_PIPET" || true)"
if [ "$first_char" != "c" ]; then
  # å»æ‰æª”é¦–å¯èƒ½å‡ºç¾çš„ç©ºç™½/ç©ºè¡Œ
  awk 'NR==1 && $0=="" {next} {print}' "$OUT_PIPET" > "$OUT_PIPET.tmp" && mv "$OUT_PIPET.tmp" "$OUT_PIPET"
fi

# --- åŸ·è¡Œ pipet ---
pipet --json -v "$OUT_PIPET" > "$OUT_JSONL"

# --- çµ±è¨ˆè¼¸å‡ºç­†æ•¸ ---
LINES="$(wc -l < "$OUT_JSONL" | awk '{print $1}')"
echo "OK: é—œéµå­—ã€Œ${KEYWORD}ã€å·²è¼¸å‡º ${LINES} ç­†åˆ° ${OUT_JSONL}ï¼ˆå…±è™•ç† ${TOTAL} é ï¼‰"
echo "pipet è¦å‰‡æª”åœ¨ï¼š$OUT_PIPET"
echo

echo "=== ç†±é–€æŠ€èƒ½è©ï¼ˆTop 20ï¼‰==="
# ====== CKIP æ–·è© + POS çš„æŠ€èƒ½è©æŠ½å–ï¼ˆå¤±æ•—è‡ªå‹• fallback æ­£å‰‡ + é™¤éŒ¯ï¼‰======
python3 -X utf8 - "$OUT_JSONL" <<'PY'
# -*- coding: utf-8 -*-
import os, sys, json, re
from collections import Counter

# å„ªå…ˆç”¨ argv[1]ï¼›æ‰¾ä¸åˆ°å†çœ‹ç’°å¢ƒè®Šæ•¸ï¼›æœ€å¾Œå˜—è©¦è‡ªå‹•å°‹æ‰¾æœ€è¿‘çš„ 104-*.jsonl
path = sys.argv[1] if len(sys.argv) > 1 else os.environ.get("OUT_JSONL")
if not path:
    import glob, os
    candidates = sorted(glob.glob("104-*.jsonl"), key=os.path.getmtime, reverse=True)
    if candidates:
        path = candidates[0]
if not path:
    print("[ERROR] æ‰¾ä¸åˆ°è¼¸å…¥æª”ï¼Œè«‹è¨­å®š OUT_JSONL æˆ–å‚³å…¥æª”ååƒæ•¸ã€‚")
    sys.exit(1)

topn = int(os.environ.get("TOPN","20"))

def walk(x):
    if isinstance(x, dict):
        yield x
        for v in x.values():
            yield from walk(v)
    elif isinstance(x, list):
        for v in x:
            yield from walk(v)

def load_any(p):
    # 1) å˜—è©¦æ•´æª”å°±æ˜¯ä¸€å€‹ JSON
    try:
        with open(p, 'r', encoding='utf-8', errors='replace') as f:
            data = json.load(f)
        print(f"[DEBUG] input={p} mode=single-JSON")
        return data
    except Exception:
        pass
    # 2) NDJSONï¼šé€è¡Œ parse
    arr, parsed = [], 0
    with open(p, 'r', encoding='utf-8', errors='replace') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                arr.append(json.loads(line))
                parsed += 1
            except Exception:
                # å¿½ç•¥åƒæ˜¯ '],' ä¹‹é¡è¡Œ
                continue
    print(f"[DEBUG] input={p} mode=ndjson parsed_lines={parsed}")
    return arr

re_html = re.compile(r"<[^>]+>")

data = load_any(path)

# æŠ½å‡ºå« title/æè¿° çš„ç‰©ä»¶
records = []
dicts_seen = 0
for d in walk(data):
    if isinstance(d, dict):
        dicts_seen += 1
        if any(k in d for k in ("title","jobName","descSnippet","descWithoutHighlight","description")):
            records.append(d)

print(f"[DEBUG] dicts_seen={dicts_seen}, job_like_dicts={len(records)}")

# çµ„èªæ–™
texts=[]
for o in records:
    title = o.get("title") or o.get("jobName") or ""
    desc  = o.get("descSnippet") or o.get("descWithoutHighlight") or o.get("description") or ""
    t = (title + " " + desc)
    t = re_html.sub(" ", t).replace("&lt;","<").replace("&gt;",">").replace("&amp;","&")
    t = t.replace("\\n"," ").replace("\n"," ")
    if t.strip():
        texts.append(t)

print(f"[DEBUG] texts_nonempty={len(texts)}")
if not texts:
    print("=== ç†±é–€æŠ€èƒ½è©ï¼ˆTop %dï¼‰[ç„¡èªæ–™]===" % topn)
    sys.exit(0)

# è©æŠ½å–ï¼šå…ˆè©¦ CKIPï¼Œå¤±æ•—å°± regex fallback
terms = None
ckip_ok = False
try:
    from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger  # type: ignore
    ws  = CkipWordSegmenter(model="bert-base")
    pos = CkipPosTagger(model="bert-base")
    ws_result  = ws(texts, batch_size=8)
    pos_result = pos(ws_result, batch_size=8)

    alpha = re.compile(r"^[A-Za-z]{2,}$")         # ç´”è‹±æ–‡ã€é•·åº¦â‰¥2
    has_digit = re.compile(r"[0-9]")              # å»æ‰å«æ•¸å­—
    terms=[]
    for toks,tags in zip(ws_result,pos_result):
        for w,t in zip(toks,tags):
            if not w or has_digit.search(w):
                continue
            # è‹±æ–‡ï¼šç´”å­—æ¯ä¸”é•·åº¦>1 â†’ å…¨è½‰å°å¯«è¨ˆæ•¸
            if alpha.match(w):
                terms.append(w.lower())
                continue
            # ä¸­æ–‡ï¼šåè© + 2~6 å€‹æ¼¢å­—
            w2 = w.strip()
            if t.startswith('N') and 2 <= len(w2) <= 6 and all('\u4e00'<=c<='\u9fff' for c in w2):
                terms.append(w2)
    ckip_ok = True
except Exception as e:
    print(f"[DEBUG] CKIP unavailable ({e}); using regex fallback.")
    alpha = re.compile(r"\b[A-Za-z]{2,}\b")
    han   = re.compile(r"[\u4e00-\u9fff]{2,6}")
    has_digit = re.compile(r"[0-9]")
    terms=[]
    for t in texts:
        # è‹±æ–‡ï¼ˆå»æ•¸å­—ã€å·²ä¿è­‰ç„¡æ¨™é»ï¼›ä»¥éå­—å…ƒåˆ‡ï¼Œå†åŒ¹é…ï¼‰
        terms += [w.lower() for w in alpha.findall(t) if not has_digit.search(w)]
        # ä¸­æ–‡ï¼ˆ2~6 é€£çºŒæ¼¢å­—ï¼‰
        terms += han.findall(t)

from collections import Counter
freq = Counter(terms)

print("=== ç†±é–€æŠ€èƒ½è©ï¼ˆTop {}ï¼‰{}===".format(topn, "" if ckip_ok else "[fallback]"))
for term, cnt in freq.most_common(topn):
    print("{:5d} {}".format(cnt, term))
PY
# ====== /æŠ€èƒ½è©ï¿½ ï¿½ ======





echo "=== è–ªè³‡çµ±è¨ˆï¼ˆä»¥ salaryLow/salaryHigh è§£æï¼Œå–®ä½ï¼šå…ƒï¼‰==="

vals="$(jq -r '
  recurse(.[]?; .) | select(type=="object")
  | [ (.salaryLow // "0"), (.salaryHigh // "0") ]
  | map(tostring | gsub(",|\\s";"") | (tonumber? // 0))
  | @tsv
' "$OUT_JSONL" | awk -F'\t' '
  {
    low=$1+0; high=$2+0;
    # å¿½ç•¥ 9999999ï¼ˆã€Œä»¥ä¸Šã€ï¼‰èˆ‡æ˜é¡¯ä¸åˆç†é«˜å€¼ï¼ˆ> 5,000,000ï¼‰
    if(high==9999999 || high>5000000) high=0;
    if(low==9999999  || low>5000000)  low=0;
    if(low+high>0) print low "\t" high;
  }' || true)"

if [[ -z "${vals}" ]]; then
  echo "ï¼ˆæ²’æœ‰å¯ç”¨çš„ salaryLow/salaryHigh æ•¸å€¼ï¼Œå¯èƒ½å¤šç‚ºã€Œå¾…é‡é¢è­°ã€ã€‚ï¼‰"
else
  printf "%s\n" "$vals" | awk -F'\t' '
    BEGIN{lc=0; lsum=0; lmin=""; lmax=0; hc=0; hsum=0; hmin=""; hmax=0}
    {
      low=$1+0; high=$2+0;
      if(low>0){lc++; lsum+=low; if(lmin==""||low<lmin){lmin=low} if(low>lmax){lmax=low}}
      if(high>0){hc++; hsum+=high; if(hmin==""||high<hmin){hmin=high} if(high>hmax){hmax=high}}
    }
    END{
      if(lc>0){ printf("LOW ï¼šæœ‰æ˜ç¢ºä¸‹é™ %d ç­†ï¼›å¹³å‡ %.0fï¼›æœ€ä½ %dï¼›æœ€é«˜ %d\n", lc, lsum/lc, lmin, lmax) }
      else    { print "LOW ï¼šç„¡æ˜ç¢ºä¸‹é™æ•¸æ“š" }
      if(hc>0){ printf("HIGHï¼šæœ‰æ˜ç¢ºä¸Šé™ %d ç­†ï¼›å¹³å‡ %.0fï¼›æœ€ä½ %dï¼›æœ€é«˜ %d\n", hc, hsum/hc, hmin, hmax) }
      else    { print "HIGHï¼šç„¡æ˜ç¢ºä¸Šé™æ•¸æ“š" }
    }'
fi
echo



echo "=== ä¸Šé™æœ€é«˜ Top 10 è·ç¼ºï¼ˆhigh descï¼‰==="
jq -c '
  recurse(.[]?; .) | select(type=="object")
  | { title:   (.title   // .jobName),
      company: (.company // .custName),
      location, salary, job_url,
      high_raw: ((.salaryHigh // "0") | tostring | gsub(",|\\s";"") | tonumber? // 0)
    }
  | select(.high_raw>0 and .high_raw!=9999999 and .high_raw<=5000000)
  | .high = .high_raw
  | del(.high_raw)
' "$OUT_JSONL" \
| jq -s 'sort_by(-.high) | .[] | .title+" | "+.company+" | "+(.high|tostring)+" | "+(.job_url // "")' \
| head -n 10 || true
